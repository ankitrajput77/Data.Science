what is ml?
- 
Machine Learning (ML) is a subfield of artificial intelligence (AI) that focuses on the development of algorithms and statistical models that enable computer systems to learn from and make predictions or decisions based on data. 
The fundamental idea behind machine learning is to allow computers to automatically learn and improve their performance on a specific task without being explicitly programmed.

what is supervised ml?
-
Supervised Machine Learning (Supervised ML) is a type of machine learning where the algorithm is trained on a labeled dataset, 
meaning that the input data is paired with corresponding output labels. The goal of supervised learning is to learn a mapping or 
relationship between the input features and the output labels so that the model can make predictions on new, unseen data.

what is unsupervised ml?
-
Unsupervised Machine Learning (Unsupervised ML) is a type of machine learning where the algorithm is given data without explicit instructions on what to do with it. 
In other words, the algorithm is not provided with labeled output data, and its goal is to find patterns, structures, or relationships within the data on its own.
- Fraud Detection in Finance
- Medical Image Analysis

data collection
data preprocessing
feature extraction and dimension reduction
clustering
anomoly detection
association rule learning

what is reinforcement learning ?
-
Reinforcement Learning (RL) is a type of machine learning paradigm where an agent learns to make decisions by interacting with its environment. 
The agent takes actions, receives feedback in the form of rewards or penalties, and adjusts its strategy to maximize the cumulative reward over time. 
The goal of reinforcement learning is to enable the agent to learn optimal behavior in a given environment.
-
example:- learning cycle, gaming bots
example:- AlphaGo

validation dataset?
-
A validation data set is a data-set of examples used to tune the hyperparameters (i.e. the architecture) of a classifier.
In order to avoid overfitting, when any classification parameter needs to be adjusted, it is necessary to have a validation data set in addition to the training and test datasets. For example, if the most suitable classifier for the problem is sought, the training data set is used to train the different candidate classifiers, the validation data set is used to compare their performances and decide which one to take and, 
finally, the test data set is used to obtain the performance characteristics such as accuracy, sensitivity, specificity, F-measure, and so on.

Underfitting and overfitting?
-
Underfitting and overfitting are two common issues in machine learning that relate to how well a model generalizes to new, unseen data.

underfitting?
-
(High Bias and Low variance)
Underfitting occurs when a model is too simple to capture the underlying patterns in the training data. As a result, it performs poorly on both the training data and new, unseen data.
reason:-
used wrong model, used linear model in non linear dataset
dataset is insufficient, small, insuficient features
less epochs.
addressing:-
use good model
increase complexity of model by hyperparameter tunning
adding more layers.
good number of epochs.

Overfitting?
(High Variance and Low Bias)
-
Overfitting occurs when a model is too complex and fits the training data too closely, capturing noise(Noisy data are data with a large amount of additional meaningless information in it called noise.) and outliers. 
As a result, it performs well on the training data but poorly on new, unseen data.
Reasons:-
Model complexity is too high, allowing the model to fit the noise in the training data.
Limited amount of training data.
addressing:-
Reduce model complexity (e.g., use simpler algorithms or regularization techniques).
Increase the amount of training data.
Use techniques like cross-validation to assess model performance on different subsets of the data.
Apply regularization methods to penalize overly complex models.

Bias and variance?
-
Bias
-
Reperesent the model tendency to consistently make incorrect assumptions about the relationship between input and target variable.

variance
-
Variance refers to the model's sensitivity to small fluctuations or noise in the training data. It measures how much the model's predictions would vary if it were trained on a different dataset.
Using a high-degree polynomial model with insufficient data, causing the model to fit the noise in the training set.


Bias-Variance Tradeoff?
-
There is often a tradeoff between bias and variance. Increasing model complexity tends to reduce bias but may increase variance, and vice versa.


MCAR - no systematic reason of missing
MAR - has relationship with observed data but not with missing data
MNAR - has relationship with missing data's factors.

smote - synthetic minority oversampling technique

Regularization techniques, such as L1 and L2 regularization, penalize large coefficients in linear models. If features are on different scales, regularization may disproportionately shrink or penalize some features more than others.
Neural networks with gradient-based optimization benefit from feature scaling. It helps avoid vanishing or exploding gradients, leading to more stable and efficient training.


1. Filter Methods:
Statistical Tests:

Use statistical tests (e.g., chi-squared, ANOVA) to evaluate the correlation between each feature and the target variable.
Calculate correlation coefficients (e.g., Pearson, Spearman) between features and remove those with low correlation with the target variable or high correlation with other features.


2. Wrapper Methods:
Recursive Feature Elimination (RFE):

Fit a model, remove the least significant feature, and repeat until the desired number of features is reached.
Forward Feature Selection:

Start with an empty set of features and iteratively add the most significant feature until the model's performance stops improving.
Backward Feature Elimination:

Start with all features and iteratively remove the least significant feature until the model's performance stops improving.


Regularization Techniques:

Apply regularization techniques (e.g., L1 or L2 regularization) that penalize large coefficients, encouraging the model to select only the most relevant features.
Principal Component Analysis (PCA):

Transform features into a lower-dimensional space while retaining most of the original data's variability.


Orthogonality means that the components are independent of each other, and each one contributes unique information to the overall variance.
3. Decorrelation:
Orthogonal axes imply that the covariance between different components is zero. This is a desirable property because it simplifies the interpretation of the components.
symmetric matrix, the eigenvalues are always real and eigenvectors corresponding to distinct eigenvalues are always orthogonal.
The principal components generated by PCA are linear combinations of the original features. While they capture the most variance in the data, the resulting components may be challenging to interpret in terms of the original features.
PCA assumes that the underlying relationships in the data are linear. If the data has nonlinear relationships, PCA may not capture them effectively.

R^2
(pronounced "R-squared") is a statistical measure of the proportion of the variance in the dependent variable that is explained by the independent variables in a regression model. It is also known as the coefficient of determination.





